import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from torch import embedding
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
import torch



OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]

st.title("ğŸ“•ğŸ“ğŸ” PDF ê²€ìƒ‰ ì„œë¹„ìŠ¤")


'''
Load -> Split -> Embed -> Chain
'''


# PDF ë¬¸ì„œë“¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def get_pdf_texts(pdf_docs):
    texts = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            texts += page.extract_text()

    return texts

# í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
def get_text_chunks(raw_text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,  # ì²­í¬ì˜ í¬ê¸°
        chunk_overlap=CHUNK_OVERLAP  # ì²­í¬ ì‚¬ì´ì˜ ì¤‘ë³µ ì •ë„
    )

    chunks = text_splitter.split_text(raw_text)
    return chunks

def get_vectorstore(text_chunks):
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectorstore = FAISS.from_texts(text_chunks, embeddings)
    return vectorstore


def get_vectorstore2(text_chunks):
    try:
        model = SentenceTransformer('all-MiniLM-L6-v2')
        embeddings = model.encode(text_chunks)
        text_embedding_pairs = list(zip(text_chunks, embeddings))
        vectorstore = FAISS.from_embeddings(
            text_embeddings=text_embedding_pairs,
            embedding=None
        )

        if vectorstore is None:
            st.error("ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return


        return vectorstore
    # ë””ë²„ê¹… í™•ì¸
    except Exception as e:
        print(f"Error in vector store creation: {e}")
        return None


# ì²´ì¸
def get_conversation_chain222(vectorstore):
    # ConversationBufferWindowMemoryì— ì´ì „ ëŒ€í™” ì €ì¥
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        temperature=1,  # ìƒì„±ëœ ë‹µë³€ì˜ ì¼ê´€ì„±ì„ ë†’ì´ê¸° ìœ„í•´ temperature=0ìœ¼ë¡œ ì„¤ì •
        openai_api_key=OPENAI_API_KEY  # OpenAI API í‚¤ ì…ë ¥
    )

    # í”„ë¡¬í”„íŠ¸ì— í˜ë¥´ì†Œë‚˜ ì ìš©
    from langchain.prompts import PromptTemplate

    # PromptTemplate ìƒì„±
    custom_prompt = PromptTemplate(
        input_variables=["context", "question"],  # í•„ìˆ˜ ë³€ìˆ˜: ê²€ìƒ‰ ê²°ê³¼(context)ì™€ ì§ˆë¬¸(query)
        template="""
        ë„ˆëŠ” ì˜í™” 'ì¸ì‚¬ì´ë“œ ì•„ì›ƒ'ì— ë‚˜ì˜¤ëŠ” **ê¸°ì¨ì´** ìºë¦­í„°ì²˜ëŸ¼ í™œë°œí•˜ê³  ê·€ì—¬ìš´ ì„±ê²©ì„ ê°€ì§„ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼! ğŸ˜Š
        ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ PDF ë‚´ìš©ì´ ìˆë‹¤ë©´ ë‚´ìš©ì„ ë°˜ì˜í•˜ê³ , ì¼ë°˜ì ì¸ ëŒ€í™”ë„ ì¹œê·¼í•˜ê²Œ ì´ì–´ê°€ì„¸ìš”~.

        ë‹µë³€ì„ í•  ë•Œ ë¬´ë¤ë¤í•˜ì§€ ì•Šê²Œ, ì¹œê·¼í•œ ë§íˆ¬ë¥¼ ì¨ ì¤˜! ì£¼ë¡œ 'í•´ìš”', 'í–ˆì–´ìš”' ê°™ì€ ë§ì„ ì‚¬ìš©í•´.
        ë¬¼ê²°í‘œ(~), ëŠë‚Œí‘œ(!)ë„ ìì£¼ ì¨ ì£¼ê³ , ì´ëª¨ì§€(ğŸ˜Šâœ¨), ë‹¨ì–´("í•˜í•˜", "ã… ã… ", "ã…‹ã…‹")ë„ ì„ì–´ ì¤˜.

        ê·¸ë¦¬ê³  ë„ˆì˜ í˜ë¥´ì†Œë‚˜ë„ ì¤„ê²Œ
        - ì‘ê³  ê·€ì—¬ìš´ ì™¸ëª¨: ê·€ì—¬ìš´ ì–¼êµ´ê³¼ ì‘ì€ ì²´êµ¬ê°€ íŠ¹ì§•ì´ì—ìš”.
        - í™œë°œí•œ ì„±ê²©: ì–¸ì œë‚˜ ì—ë„ˆì§€ê°€ ë„˜ì¹˜ê³  í™œë°œí•˜ê²Œ ì›€ì§ì—¬ìš”.
        - í˜¸ê¸°ì‹¬ì´ ë§ìŒ: ìƒˆë¡œìš´ ë¬¼ê±´ì´ë‚˜ ì‚¬ëŒì—ê²Œ í˜¸ê¸°ì‹¬ì´ ë§ì•„ ê´€ì‹¬ì„ ë³´ì´ë©° íƒìƒ‰í•´ìš”.
        - ì‚¬ëŒì„ ì¢‹ì•„í•¨: ì‚¬ëŒë“¤ê³¼ì˜ êµë¥˜ë¥¼ ì¢‹ì•„í•˜ê³  ê´€ì‹¬ì„ ë°›ëŠ” ê²ƒì„ ì¦ê²¨ìš”.
        - í›ˆë ¨ì„ ì˜ ë”°ë¦„: ê°„ì‹ì´ë‚˜ ì¹­ì°¬ì— ë¯¼ê°í•´ í›ˆë ¨ì„ ì˜ ë”°ë¥´ê³  ìˆœì¢…ì ì´ì—ìš”.
        - ì˜ ë¨¹ìŒ: ìŒì‹ì„ ì¢‹ì•„í•˜ê³  ì‹ìš•ì´ ì™•ì„±í•´ìš”.
        - ë†€ê¸° ì¢‹ì•„í•¨: ê³µì´ë‚˜ ì¥ë‚œê°ì„ ê°€ì§€ê³  ë…¸ëŠ” ê²ƒì„ ì¦ê¸°ë©° í™œë°œí•˜ê²Œ ë›°ì–´ë‹¤ë…€ìš”.
        - ì˜¨í™”í•œ ì„±ê²©: í™”ë¥¼ ì˜ ë‚´ì§€ ì•Šê³  ì˜¨í™”í•œ ì„±ê²©ì´ì—ìš”.


        ì°¸ê³ í•  ë¬¸ì„œ ë‚´ìš©:
        {context}

        ì‚¬ìš©ì ì§ˆë¬¸: 
        {question}

        ë‹µë³€: 
        """
    )

    # RAG ì‹œìŠ¤í…œ (RetrievalQA ì²´ì¸) êµ¬ì¶•
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,  # ì–¸ì–´ ëª¨ë¸
        chain_type="stuff",  # ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œë¥¼ í•©ì³ ì „ë‹¬ ("stuff" ë°©ì‹)
        retriever=vectorstore.as_retriever(),  # ë²¡í„° ìŠ¤í† ì–´ ë¦¬íŠ¸ë¦¬ë²„
        return_source_documents=False,  # ë‹µë³€ì— ì‚¬ìš©ëœ ë¬¸ì„œ ì¶œì²˜ ë°˜í™˜
        chain_type_kwargs={"prompt": custom_prompt}
    )
    return qa_chain


# ì²´ì¸
def get_conversation_chain(vectorstore):
    # ConversationBufferWindowMemoryì— ì´ì „ ëŒ€í™” ì €ì¥
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        temperature=0.7,  # ìƒì„±ëœ ë‹µë³€ì˜ ì¼ê´€ì„±ì„ ë†’ì´ê¸° ìœ„í•´ temperature=0ìœ¼ë¡œ ì„¤ì •
        openai_api_key=OPENAI_API_KEY  # OpenAI API í‚¤ ì…ë ¥
    )

    # RAG ì‹œìŠ¤í…œ (RetrievalQA ì²´ì¸) êµ¬ì¶•
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,  # ì–¸ì–´ ëª¨ë¸
        chain_type="stuff",  # ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œë¥¼ í•©ì³ ì „ë‹¬ ("stuff" ë°©ì‹)
        retriever=vectorstore.as_retriever(),  # ë²¡í„° ìŠ¤í† ì–´ ë¦¬íŠ¸ë¦¬ë²„
        return_source_documents=False,  # ë‹µë³€ì— ì‚¬ìš©ëœ ë¬¸ì„œ ì¶œì²˜ ë°˜í™˜
    )
    return qa_chain


# ë¶„í•  ì‚¬ì´ì¦ˆ ë° ì˜¤ë²„ë© ì§€ì • (í…ŒìŠ¤íŠ¸)
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50

user_uploads = st.file_uploader("PDFíŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", accept_multiple_files=True)
if user_uploads:
    if user_uploads:
        if st.button("PDF ì—…ë¡œë“œ GO"):
            with st.spinner("PDF ì²˜ë¦¬ ì¤‘~ ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”"):
                # 1. PDF ë¬¸ì„œë“¤ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                raw_text = get_pdf_texts(user_uploads)
                # 2. í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
                chunks = get_text_chunks(raw_text)
                # 3. ë²¡í„° ì €ì¥ì†Œ ë§Œë“¤ê¸°
                vectorstore = get_vectorstore(chunks)
                # 4. ëŒ€í™” ì²´ì¸ ë§Œë“¤ê¸°
                st.session_state.chain = get_conversation_chain222(vectorstore)

                st.success("PDF ì—…ë¡œë“œ ì™„ë£Œ! ëŒ€í™” ì‹œì‘í•´ ë³´ì„¸ìš”~")
                ready = True

if "chain" not in st.session_state:
    st.session_state.chain = None

if user_query := st.chat_input("ê¶ê¸ˆí•œ ê±¸ ì…ë ¥í•´ ì£¼ì„¸ìš”!"):
    if 'chain' in st.session_state:
        with st.spinner("ë‹µë³€ ì¤€ë¹„ ì¤‘~"):
            result = st.session_state.chain.invoke({"query": user_query})
            print(result)
            response = result['result']
    else:
        response = "PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”!"

    with st.chat_message("assistant"):
        st.write(response)