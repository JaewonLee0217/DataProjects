import streamlit as st
from PIL import Image
from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

# OpenAI API í‚¤ ì„¤ì •
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]

# GPT-4 ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(
    model_name="gpt-4",
    temperature=0.7,
    openai_api_key=OPENAI_API_KEY
)

# CLIP ëª¨ë¸ ì´ˆê¸°í™”
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# BLIP ëª¨ë¸ ì´ˆê¸°í™”
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")

# Streamlit ì œëª©
st.title("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì„ íƒ ë° ë¶„ì„ ì„œë¹„ìŠ¤")

# ì´ë¯¸ì§€ ì—…ë¡œë“œ ì„¹ì…˜
uploaded_files = st.file_uploader("ë‘ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "jpeg", "png"], accept_multiple_files=True)

if len(uploaded_files) == 2:
    # ì´ë¯¸ì§€ ë¡œë“œ ë° í‘œì‹œ
    images = [Image.open(file) for file in uploaded_files]
    st.image(images, caption=["ì´ë¯¸ì§€ 1", "ì´ë¯¸ì§€ 2"], use_column_width=True)

    # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì…ë ¥
    user_prompt = st.text_input("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'ìì—° í’ê²½ì´ í¬í•¨ëœ ì´ë¯¸ì§€')")

    if user_prompt and st.button("ë¶„ì„ ì‹œì‘"):
        with st.spinner("ì´ë¯¸ì§€ ë¶„ì„ ì¤‘..."):
            # CLIP ëª¨ë¸ë¡œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
            inputs = clip_processor(text=[user_prompt], images=images, return_tensors="pt", padding=True)
            outputs = clip_model(**inputs)
            logits_per_image = outputs.logits_per_image
            probs = logits_per_image.softmax(dim=1).tolist()

            # BLIP ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ìº¡ì…”ë‹
            captions = []
            for image in images:
                inputs = blip_processor(image, return_tensors="pt")
                out = blip_model.generate(**inputs)
                caption = blip_processor.decode(out[0], skip_special_tokens=True)
                captions.append(caption)

            # ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ì´ë¯¸ì§€ ì„ íƒ
            selected_index = probs[0].index(max(probs[0]))
            selected_image = images[selected_index]

            # GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ ì„¤ëª… ìƒì„±
            prompt_template = PromptTemplate(
                input_variables=["user_prompt", "image1_caption", "image2_caption", "selected_image", "reasoning"],
                template="""
                ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸: {user_prompt}

                ì´ë¯¸ì§€ 1 ìº¡ì…˜: {image1_caption}
                ì´ë¯¸ì§€ 2 ìº¡ì…˜: {image2_caption}

                ì„ íƒëœ ì´ë¯¸ì§€: {selected_image}

                ì„ íƒ ì´ìœ :
                {reasoning}
                """
            )

            # GPT-4ë¡œ ì„¤ëª… ìƒì„±
            reasoning_input = {
                "user_prompt": user_prompt,
                "image1_caption": captions[0],
                "image2_caption": captions[1],
                "selected_image": f"ì´ë¯¸ì§€ {selected_index + 1}",
                "reasoning": (
                    f"CLIP ëª¨ë¸ ë¶„ì„ ê²°ê³¼, ì…ë ¥ëœ í”„ë¡¬í”„íŠ¸ '{user_prompt}'ì™€ ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ì´ë¯¸ì§€ëŠ” "
                    f"ì´ë¯¸ì§€ {selected_index + 1}ì…ë‹ˆë‹¤. ì´ ì´ë¯¸ì§€ëŠ” í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œëœ íŠ¹ì§•ì„ ë” ì˜ ë°˜ì˜í•©ë‹ˆë‹¤."
                ),
            }
            explanation = llm.predict(prompt_template.format(**reasoning_input))

        # ê²°ê³¼ ì¶œë ¥
        st.success("ë¶„ì„ ì™„ë£Œ!")
        st.image(selected_image, caption=f"ì„ íƒëœ ì´ë¯¸ì§€ (ì´ë¯¸ì§€ {selected_index + 1})", use_column_width=True)
        st.write("## ë¶„ì„ ê²°ê³¼")
        st.write(f"**ì´ë¯¸ì§€ 1 ìº¡ì…˜:** {captions[0]}")
        st.write(f"**ì´ë¯¸ì§€ 2 ìº¡ì…˜:** {captions[1]}")
        st.write("## GPT-4 ì„¤ëª…")
        st.write(explanation)
else:
    st.warning("ë‘ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
